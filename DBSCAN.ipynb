{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hAt5uDXQSrX"
      },
      "outputs": [],
      "source": [
        "# For Google Colab integration\n",
        "import os\n",
        "from google.colab import drive\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import data as dataframe\n",
        "file_path = '/content/drive/MyDrive/Infor648/Data/Mall_Customers.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# calling head() method\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2BHhOW8VRMNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select variables of interest"
      ],
      "metadata": {
        "id": "k-IQhbWHXhA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub= df[['Annual Income (k$)', 'Spending Score (1-100)']]"
      ],
      "metadata": {
        "id": "Pcxo8hBRSBko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization or standardization"
      ],
      "metadata": {
        "id": "859OOfeUXlMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#data standardization\n",
        "scaler = StandardScaler()\n",
        "data_std = scaler.fit_transform(df_sub)"
      ],
      "metadata": {
        "id": "sRp_fptVT6LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#data normalization\n",
        "norm_scaler = MinMaxScaler()\n",
        "data_norm = norm_scaler.fit_transform(df_sub)\n",
        "##data_norm is the normalized data"
      ],
      "metadata": {
        "id": "16N3e_UeSEQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "model = DBSCAN(eps=0.1, min_samples=5)\n",
        "model.fit(data_norm)\n",
        "df_sub['cluster'] = model.fit_predict(data_norm)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HZRsQMRBRXQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Colors for clusters\n",
        "unique_clusters = df_sub['cluster'].unique()\n",
        "colors = plt.cm.get_cmap('twilight', len(unique_clusters))\n",
        "\n",
        "for cluster in unique_clusters:\n",
        "    if cluster == -1:\n",
        "        # Noise points (outliers)\n",
        "        plt.scatter(data_norm[df_sub['cluster'] == cluster, 0],\n",
        "                    data_norm[df_sub['cluster'] == cluster, 1],\n",
        "                    color='k', label='Outliers')\n",
        "    else:\n",
        "        # Plot clusters\n",
        "        plt.scatter(data_norm[df_sub['cluster'] == cluster, 0],\n",
        "                    data_norm[df_sub['cluster'] == cluster, 1],\n",
        "                    color=colors(cluster), label=f'Cluster {cluster}')\n",
        "\n",
        "plt.title('DBSCAN Clustering')\n",
        "plt.xlabel('Normalized Annual Income (k$)')\n",
        "plt.ylabel('Normalized Spending Score (1-100)')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(False)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I7qJrhWFXN90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation\n",
        "#Low Outlier Ratio (< 5%)\n",
        "#Moderate Outlier Ratio (5% - 20%)\n",
        "#High Outlier Ratio (>20%)\n",
        "\n",
        "###Which one is a good outlier ratio highly depends on your specific dataset and problem domain"
      ],
      "metadata": {
        "id": "IrPvw4qQewM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##you do not need to change anything here\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def calculate_dbscan_metrics(data_norm, eps, min_samples):\n",
        "    # Fit DBSCAN model\n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = model.fit_predict(data_norm)\n",
        "\n",
        "    # Number of clusters (excluding outlier)\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "    # Outlier ratio\n",
        "    outlier_ratio = np.sum(labels == -1) / len(labels)\n",
        "\n",
        "    # Calculate SSE (Sum of Squared Errors) within clusters\n",
        "    sse = 0\n",
        "    cluster_centers = {}\n",
        "    for cluster_label in set(labels):\n",
        "        if cluster_label == -1:\n",
        "            continue\n",
        "        cluster_points = data_norm[labels == cluster_label]\n",
        "        cluster_center = np.mean(cluster_points, axis=0)\n",
        "        cluster_centers[cluster_label] = cluster_center\n",
        "        sse += np.sum(np.linalg.norm(cluster_points - cluster_center, axis=1) ** 2)\n",
        "\n",
        "    # Calculate SSB (Sum of Squared Between-cluster)\n",
        "    overall_mean = np.mean(data_norm, axis=0)\n",
        "    ssb = 0\n",
        "    for cluster_label, center in cluster_centers.items():\n",
        "        cluster_size = np.sum(labels == cluster_label)\n",
        "        ssb += cluster_size * np.sum((center - overall_mean) ** 2)\n",
        "\n",
        "    # Total SSE + SSB\n",
        "    sse_ssb_sum = sse + ssb\n",
        "\n",
        "    # Calculate Silhouette Score (ignoring outliers)\n",
        "    if n_clusters > 1:\n",
        "        silhouette_avg = silhouette_score(data_norm[labels != -1], labels[labels != -1])\n",
        "    else:\n",
        "        silhouette_avg = np.nan  # Silhouette score is undefined for 1 or no clusters\n",
        "\n",
        "    # Create DataFrame to display the results\n",
        "    results = {\n",
        "        'eps': [eps],\n",
        "        'min_samples': [min_samples],\n",
        "        'n_clusters': [n_clusters],\n",
        "        'outlier_ratio': [outlier_ratio],\n",
        "        'SSE': [sse],\n",
        "        'SSB': [ssb],\n",
        "        'SSE + SSB': [sse_ssb_sum],\n",
        "        'Silhouette Score': [silhouette_avg]\n",
        "    }\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "aFCajbXBVK-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Consider other possible eps_value and min_sample_value"
      ],
      "metadata": {
        "id": "xN-aPC96VTFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter ranges\n",
        "# Create an array of eps values starting from 0.1 to 1.0 (exclusive), with a step size of 0.1.\n",
        "# This means eps will take values like 0.1, 0.2, 0.3, ..., up to 0.9.\n",
        "eps_values = np.arange(0.1, 1.0, 0.1) # change eps_value here\n",
        "min_samples_values = np.arange(3, 10) # change min_sample here\n",
        "\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Loop through each combination of eps and min_samples\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        # Apply DBSCAN and calculate metrics\n",
        "        df_metrics = calculate_dbscan_metrics(data_norm, eps, min_samples)\n",
        "        results.append(df_metrics)\n",
        "\n",
        "# Concatenate all results\n",
        "results_df = pd.concat(results, ignore_index=True)\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "\n",
        "# Sort by SSE + SSB\n",
        "results_df_sorted = results_df.sort_values(by='SSE + SSB', ascending=True)\n",
        "display(results_df_sorted)\n",
        "\n",
        "# Sort by Silhouette Score\n",
        "results_df_sorted = results_df.sort_values(by='Silhouette Score', ascending=False)\n",
        "display(results_df_sorted)\n"
      ],
      "metadata": {
        "id": "tXgON73AYQ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rerun the model with better eps and min_samples"
      ],
      "metadata": {
        "id": "iH3P0psBVwhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.1\n",
        "min_samples = 9\n",
        "df_metrics = calculate_dbscan_metrics(data_norm, eps, min_samples)\n",
        "\n",
        "# Display the metrics DataFrame\n",
        "display(df_metrics)"
      ],
      "metadata": {
        "id": "8p6S3o_CT44g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = 0.1\n",
        "min_samples = 8\n",
        "df_metrics = calculate_dbscan_metrics(data_norm, eps, min_samples)\n",
        "\n",
        "# Display the metrics DataFrame\n",
        "display(df_metrics)"
      ],
      "metadata": {
        "id": "W_-Xwsh_WhMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###You do not need to change anything here\n",
        "def generate_cluster_profile_dbscan(data_norm, eps, min_samples, df_original):\n",
        "    # Apply DBSCAN with customizable eps and min_samples\n",
        "    dbscan_stats = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    df_sub = df_original.copy()\n",
        "    df_sub['cluster'] = dbscan_stats.fit_predict(data_norm)\n",
        "\n",
        "    # Create a copy of the DataFrame for calculations\n",
        "    df_calculate = df_sub.copy()\n",
        "    df_calculate['cluster_result'] = 'Cluster ' + (df_calculate['cluster']).astype(str)\n",
        "\n",
        "    # Exclude the 'cluster' column for mean calculation\n",
        "    df_mean_calculation = df_calculate.drop(columns=['cluster'])\n",
        "    df_mean_feature = df_mean_calculation.drop(columns=['cluster_result'])\n",
        "\n",
        "    # Calculate Overall Mean for All Features in df_mean_feature\n",
        "    overall_means = df_mean_feature.mean().to_frame().T\n",
        "    overall_means.index = ['Overall']\n",
        "\n",
        "\n",
        "    df_cluster_summary = df_mean_calculation.groupby('cluster_result').mean()\n",
        "\n",
        "\n",
        "    df_profile = pd.concat([df_cluster_summary, overall_means], axis=0)\n",
        "\n",
        "    # Calculate the count of items in each cluster\n",
        "    cluster_counts = df_calculate['cluster_result'].value_counts()\n",
        "\n",
        "    # Calculate the percentage of items in each cluster\n",
        "    cluster_percentages = (cluster_counts / cluster_counts.sum()) * 100\n",
        "\n",
        "    # Create a DataFrame with counts and percentages\n",
        "    df_count_percentage = pd.DataFrame({\n",
        "        'Count': cluster_counts,\n",
        "        'Percentage': cluster_percentages\n",
        "    })\n",
        "\n",
        "    # Add a row for \"Overall\"\n",
        "    df_count_percentage.loc['Overall'] = [len(df_calculate), 100.0]\n",
        "    df_profile = pd.concat([df_profile, df_count_percentage], axis=1)\n",
        "    df_overall = df_profile.loc['Overall']\n",
        "    df_profile = df_profile.drop(index='Overall')\n",
        "\n",
        "    # Sort the clusters by the Count column\n",
        "    df_profile = df_profile.sort_values(by='Count', ascending=False)\n",
        "\n",
        "    # Rename 'Cluster -1' to 'Outliers' in the final DataFrame\n",
        "    df_profile.index = df_profile.index.str.replace('Cluster -1', 'Outliers')\n",
        "\n",
        "    # Append the \"Overall\" row back to the sorted DataFrame\n",
        "    df_profile = pd.concat([df_profile, df_overall.to_frame().T])\n",
        "\n",
        "    # Format the profile DataFrame\n",
        "    df_profile = df_profile.style.format({\n",
        "        \"Count\": \"{:.0f}\",\n",
        "        **{col: \"{:.2f}\" for col in df_profile.columns if col != \"Count\"}  # Two decimal places for all other columns\n",
        "    }).background_gradient(cmap='Purples')  ###### Change color here to 'Blues', 'Purples', 'Oranges', etc.\n",
        "\n",
        "    return df_profile\n",
        "\n"
      ],
      "metadata": {
        "id": "nLC58xEoRrgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_norm is our normalized data, eps = 0.1 amd min_samples = 8 df_sub is our selected subsect dataset with features of interest\n",
        "profile_1 = generate_cluster_profile_dbscan(data_norm, eps=0.1, min_samples=8, df_original=df_sub)\n",
        "display(profile_1)\n",
        "print()\n",
        "\n",
        "\n",
        "profile_2 = generate_cluster_profile_dbscan(data_norm, eps=0.1, min_samples=9, df_original=df_sub)\n",
        "display(profile_2)\n"
      ],
      "metadata": {
        "id": "cDsmLttsepF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}